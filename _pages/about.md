---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am currently a Master's student in Information and Communication Engineering at [Northwestern Polytechnical University (NPU)](https://www.nwpu.edu.cn/). I am a member of the Intelligent Optoelectronic Information Detection and Processing Group within the School of Electronics and Information, where I am supervised by [Prof. Yifan Zhang](https://teacher.nwpu.edu.cn/zhangyifan) and [Prof. Shaohui Mei](https://teacher.nwpu.edu.cn/meishaohui). My research primarily focuses on **computer vision**, with specific interests in **image super-resolution** and **image generation**.

I earned my Bachelor's degree in **Telecommunication Engineering** from the Faculty of Information Science and Engineering at **[Ocean University of China (OUC)](https://www.ouc.edu.cn/)**.

To date, I have submitted 0 papers to SCI journals, applied for 0 invention patents, and been authorized 0 utility model patents and 0 software copyrights.

<a href="https://murhyimgur.oss-cn-beijing.aliyuncs.com/attaches/houjinliang-cv.pdf" style="display: block; width: 25%; padding: 8px 0; background-color: #f2f3f3; color: #444; text-decoration: none; border-radius: 4px; font-weight: 600; font-size: 14px; border: 1px solid #e5e5e5; transition: all 0.3s ease; text-align: center;">  Download CV üìë </a>


<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->

# üìñ Educations
- *2023.09 - 2026.03 (expected)*, Northwestern Polytechnical University, Xi'an, Information and Communication Engineering. 
- *2019.09 - 2023.06*, Ocean University of China, Qingdao, Telecommunication Engineering.

# üî• News
- *2025.08*: &nbsp;üéâüéâ News02
- *2025.03*: &nbsp;üéâüéâ News01

# üìù Publications 

## üìÉ Papers

<!-- Paper 2025.03 -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> IEEE Transactions on Geoscience and Remote Sensing </div><img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**TODO**](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36)

**Jinliang Hou**, Yifan Zhang, Shaohui Mei.

- description
- description
- description
</div>
</div>


<!-- Paper 2025.03 -->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> 2025 IEEE International Geoscience and Remote Sensing Symposium</div><img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[**HSCT: HIERARCHICAL SELF-CALIBRATION TRANSFORMER FOR HYPERSPECTRAL IMAGE SUPER-RESOLUTION**](https://www.2025.ieeeigarss.org/papers/accepted_papers.php)

**Jinliang Hou**, Yifan Zhang, Yuanjie Zhi, Rugui Yao, Shaohui Mei.

- description
- description
- description

</div>
</div>


<!-- - <code class="language-plaintext highlighter-rouge">INT J MED ROBOT COMP</code> Wenda Xu, Zhihang Tan, Zexin Cao, **Haofei Ma**, Gongcheng Wang, Han Wang, Weidong Wang, Zhijiang Du. "[**DP4AuSu: Autonomous Surgical Framework for Suturing Manipulation Using Diffusion Policy with Dynamic Time Wrapping-based Locally Weighted Regression**](https://doi.org/10.1002/rcs.70072)." *The International Journal of Medical Robotics and Computer Assisted Surgery* (2025). doi: [10.1002/rcs.70072](https://doi.org/10.1002/rcs.70072)

- <code class="language-plaintext highlighter-rouge">Int J Adv Manuf Tech</code> **Haofei Ma**, Gongcheng Wang, Hua Bai, Zhiyu Xia, Weidong Wang, and Zhijiang Du. "[**Robotic Grasping Method with 6D Pose Estimation and Point Cloud Fusion**](https://doi.org/10.1007/s00170-024-14372-3)." *The International Journal of Advanced Manufacturing Technology* (2024): 1-11. doi: [10.1007/s00170-024-14372-3](https://doi.org/10.1007/s00170-024-14372-3)

- <code class="language-plaintext highlighter-rouge">RAS</code> Gongcheng Wang, **Haofei Ma**, Han Wang, Pengchao Ding, Hua Bai, Wenda Xu, Weidong Wang, and Zhijiang Du. "[**Reactive mobile manipulation based on dynamic dual-trajectory tracking**](https://doi.org/10.1016/j.robot.2023.104589)." *Robotics and Autonomous Systems* 172 (2024): 104589. doi: [10.1016/j.robot.2023.104589](https://doi.org/10.1016/j.robot.2023.104589).

- <code class="language-plaintext highlighter-rouge">IEEE Sensors Journal</code> Zhiyu Xia, Han Wang, Yulong Men, **Haofei Ma**, Zexin Cao, Weidong Wang, Zhijiang Du. "[**Kalman Filter-based EM-optical Sensor Fusion for Bone Needle Position Tracking**](https://doi.org/10.1109/JSEN.2024.3364701)." *IEEE Sensors Journal* (2024). doi: [10.1109/JSEN.2024.3364701](https://doi.org/10.1109/JSEN.2024.3364701)

- <code class="language-plaintext highlighter-rouge">RAS</code> Hua Bai, Wenrui Gao, **Haofei Ma**, Pengchao Ding, Gongcheng Wang, Wenda Xu, Weidong Wang, Zhijiang Du. "[**A study of robotic search strategy for multi-radiation sources in unknown environments**](https://doi.org/10.1109/JSEN.2024.3364701)." *Robotics and Autonomous Systems* 169 (2023): 104529. doi: [10.1109/JSEN.2024.3364701](https://doi.org/10.1109/JSEN.2024.3364701). -->



<!-- 
## üìö Patents

- <code class="language-plaintext highlighter-rouge">Invention Patent</code> [**A Rock Core Box Handling Robot**](https://cprs.patentstar.com.cn/Search/Detail?ANE=9DIE1BAA2AAA8CDA8EDA9CIB9BIF9GBC9BED6BDA9HBH9IBE), Weidong Wang, Hengbin Liang, **Haofei Ma**, Gongcheng Wang (CN202310547284.5, Pending)

- <code class="language-plaintext highlighter-rouge">Utility Model Patent</code> [**A Spherical Metamorphic Robot and An Environmental Information Monitoring System**](https://cprs.patentstar.com.cn/Search/Detail?ANE=AHIA8FDA8AGA9GGE9HAA6GAA9HDD9CIC9FCA9HDC9GDF9ICF), Yuhan Rao, Manhong Li, *Haofei Ma*, Yuchong Gao, Nuo Zhang, Xinyu Liu (CN202120212154.2)

- <code class="language-plaintext highlighter-rouge">Software Copyright</code> [**Identity Recognition and Infrared Temperature Measurement Control System**](https://register.ccopyright.com.cn/publicInquiry.html?type=softList&registerNumber=2021SR1391064&keyWord=%E9%A9%AC%E6%B5%A9%E9%A3%9E&publicityType=ALL&registerDateType=ALL), **Haofei Ma** (2021SR1391064) 

-->

# üîé Projects 


<!-- Project06 -->
## Research on Collaborative Multi-Modal Intelligent Recognition and Tracking Technology for XX

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2024.06 - 2025.06</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%">
</div>
<div class='paper-box-text' markdown="1">
This project focuses on the research of intelligent target recognition and tracking technology in multi-platform, multi-modal collaborative scenarios, aiming to enhance the system's intelligent perception and decision-making capabilities in complex environments. By introducing deep learning-based object detection and multi-object tracking algorithms, we achieve efficient fusion and processing of multi-source heterogeneous data, supporting stable operation on resource-constrained platforms. This provides critical technical support for multi-platform collaborative operations and intelligent monitoring.
</div><div markdown="1">

**Finished Works**:
1. We integrate multi-platform and multi-modal data sources to construct a unified object detection and tracking processing framework, realizing information fusion and enhanced perception.
2. For different platform resource constraints, we implement and deploy mainstream detection algorithms such as RTMDet and Deformable DETR, balancing detection accuracy with computational efficiency.
3. We implement advanced tracking algorithms like DeepSORT and ByteTrack to achieve stable target association and trajectory maintenance in complex scenarios.
4. Algorithm optimization and deployment tests are conducted on various computing platforms to verify the system's real-time performance, robustness, and collaborative perception capabilities across multiple scenarios.
</div>
</div>


<!-- Project05 -->
## Research on Pre-trained Transformer Models for 3D Pose Estimation

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2024.04 - 2024.06</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%">
</div>
<div class='paper-box-text' markdown="1">
This project aims to implement and evaluate deep learning-based methods for 3D human pose estimation. By reproducing the recently proposed MotionBERT model, training and testing it on the Human3.6M dataset, we explore its performance in 3D pose reconstruction. This method utilizes a combination of pre-training and fine-tuning to recover human pose in 3D space from 2D skeleton sequences, showing broad prospects in applications such as action recognition, human-computer interaction, and virtual reality.
</div><div markdown="1">

**Finished Works**:
1. We conducted in-depth research into mainstream methods in 3D human pose estimation, ultimately selecting MotionBERT as our experimental model. We thoroughly understood its pre-training + fine-tuning framework and its DSTformer encoder structure.
2. We reproduced the MotionBERT method on the Human3.6M dataset, implementing the model's training and testing pipeline, including data preprocessing, network architecture configuration, and training parameter settings.
3. The experimental results of MotionBERT were compared against mainstream methods such as VideoPose3D, UGCN, and PoseFormer, using MPJPE (Mean Per Joint Position Error) as the evaluation metric to validate its superiority.
4. Finally, we visualized and analyzed the experimental results, highlighting MotionBERT's advantages in terms of pose estimation accuracy and robustness, while also discussing its limitations and potential future optimization directions.
</div>
</div>



<!-- Project04 -->
## Airborne Fire Control Radar Anti-Jamming Performance Simulation System

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2023.10 - 2024.05</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%">
</div>
<div class='paper-box-text' markdown="1">
This project focuses on enhancing the anti-jamming capabilities of airborne fire control radar by designing and implementing an aeronautical electronic countermeasures simulation system. The system includes performance evaluation functionalities, aiming to provide a realistic and controllable simulation environment for aviation training and combat research. By building a multi-aircraft collaborative detection and tracking module, the system supports radar performance simulation, model testing, and algorithm expansion in multi-target scenarios, ultimately helping to improve the radar system's adaptability in real-world combat situations.
</div><div markdown="1">

**Finished Works**:
1. We've established a system framework with anti-jamming modeling and simulation capabilities, supporting the simulation and evaluation of various electronic jamming scenarios.
2. A task-oriented evaluation mechanism has been introduced to enable quantitative analysis of radar anti-jamming performance in different operational scenarios.
3. The system simulates multi-platform collaborative detection scenarios, allowing for the tracking and localization of multiple targets. This provides experimental support for the design of collaborative radar networks.
4. Finally, the system supports comparisons of target tracking models, optimization of radar network configurations, and the integration of fusion estimation algorithms, enhancing its research and validation capabilities.
</div>
</div>


<!-- Project03 -->
## Research on Hyperspectral Image Classification Techniques under Few-Shot Conditions

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2022.12 - 2023.05</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%">
</div>
<div class='paper-box-text' markdown="1">
This project addresses the challenge of hyperspectral image (HSI) classification under few-shot conditions, specifically tackling the high cost of HSI data annotation and the limited availability of training samples. We propose a method based on meta-learning and a deep relation network. By building a "learn to learn" model framework, our approach enables efficient and accurate pixel-wise classification with only a few labeled samples, thereby enhancing the practicality and applicability of hyperspectral images in resource-constrained scenarios.
</div><div markdown="1">

**Finished Works**:
1. We aim to construct a few-shot learning model suitable for hyperspectral images, improving the model's generalization ability through a meta-learning strategy, enabling it to perform classification tasks with limited labeled samples.
2. In the feature extraction phase, we utilize 3D convolution operations to extract joint spatial and spectral features from HSIs, thoroughly leveraging the structural information within hyperspectral images.
3. Based on the extracted spatial-spectral features, we use 2D convolution to compute relation scores between samples, constructing a relation module to achieve pixel-wise class inference.
4. Experiments conducted on multiple public hyperspectral datasets demonstrate that the proposed method achieves high classification accuracy even with a limited number of samples, exhibiting strong robustness and generalizability.
</div>
</div>

<!-- Project02 -->
## 2022 Mathematical Contest in Modeling (MCM)

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2022.02</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="100%">
</div>
<div class='paper-box-text' markdown="1">
In the 2022 Mathematical Contest in Modeling (MCM), our team conducted modeling research centered on "Global Equity Assessment." The project aimed to construct a comprehensive and quantifiable indicator system to measure countries' performance in terms of global equity, and to analyze the impact of resource development (such as asteroid mining) on this system. Through multi-level modeling methods, we provided quantitative support for global equity analysis and policy formulation.
</div><div markdown="1">

**Finished Works**:
1. Utilized the Analytic Hierarchy Process (AHP) to construct a global equity assessment indicator system, proposing the Global Equity Index (GIEI) and calculating scores for each country.
2. Applied the K-means++ clustering algorithm to classify countries based on their GIEI scores, identifying differences and distribution characteristics of equity levels.
3. Incorporated resource development, especially mining revenue, into an economic model, establishing a correlation model between economic growth and equity.
4. Conducted sensitivity analysis on key parameters within the model to evaluate the impact of different factors on the GIEI and country classification results, thereby enhancing the model's robustness and explanatory power.
</div>
</div>


<!-- Project01 -->
## Research on Semantic Segmentation of Mariana Trench Remote Sensing Terrain Images Based on PyTorch Semantic Segmentation

<div class='paper-box'>
<div class='paper-box-image'>
<div class="badge">2021.10 - 2021.12</div>
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="50%">
<img src='https://murphyimg.oss-cn-beijing.aliyuncs.com/img/202505302014947.png' alt="sym" width="50%">
</div>
<div class='paper-box-text' markdown="1">
The Mariana Trench presents a complex terrain that traditional image processing methods struggle to analyze effectively. Semantic segmentation offers a novel approach to understanding remote sensing imagery of such regions. This project leverages PSPNet, PSANet, and ResNet backbone networks, combined with transfer learning and data augmentation, to achieve high-precision terrain segmentation even with limited data. This capability is crucial for advancing deep-sea geomorphological analysis.
</div><div markdown="1">

**Finished Works**:
1. This project focuses on high-precision scene segmentation of remote sensing terrain images of the Mariana Trench region, utilizing the PyTorch framework.
2. We employ ResNet-50, ResNet-101, and ResNet-152 as backbone networks to construct PSPNet and PSANet models. These models are then pre-trained on public datasets such as ADE20K, PASCAL VOC 2012, and Cityscapes.
3. Subsequently, we address the limited dataset of Mariana Trench terrain images by employing various data augmentation techniques to expand the sample size. The models are then fine-tuned on this target dataset.
4. Experimental results demonstrate effective segmentation of complex terrain structures, achieving high accuracy and robustness in terrain scene parsing tasks.
</div>
</div>


# üèÜ Honors and Awards

## üèÖ Honors
- *2024.09*, Northwestern Polytechnical University Master's Second-Class Scholarship
- *2023.09*, Northwestern Polytechnical University Master's First-Class Scholarship
- *2022.09*, Ocean University of China Comprehensive Third-Class Scholarship
- *2021.09*, Ocean University of China Social Practice Scholarship

## üéè Competitions
- *2022.05*, Mathematical Contest in Modeling (MCM) Meritorious Winner
- *2020.12*, Shandong Province College Student Physics Competition Third Prize

# üíº Societies
- *2024.12*, Reviewer for IEEE Transactions on Geoscience and Remote Sensing
- *2024.11*, The 5th China International SAR Symposium (CISS 2024), Xi'an, China
- *2024.05*, Chinese Congress on Image and Graphics (CCIG 2024), Xi'an, China

<!-- # üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

<!-- # üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China. -->